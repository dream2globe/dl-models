{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습에 필요한 하이퍼파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'resnet18'\n",
    "MLP_HIDDEN_SIZE = 512\n",
    "PROJECTION_SIZE = 128\n",
    "NUM_WORKER = 0\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 123\n",
    "NUM_CLASS = 10\n",
    "LOG_EVERY_N_STEPS = 20\n",
    "MOMENTUM = 0.996\n",
    "GPU_INDEX = 0\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str = './data',\n",
    "                 mode: str = 'train'):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        \n",
    "        # 저장 경로 폴더 없을 때 만들기\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_transform(size: int = None,\n",
    "                      s: int = 1):\n",
    "        \"\"\"\n",
    "        Return a set of data augmentation transformations \n",
    "        as described in the SimCLR paper.\n",
    "        \"\"\"\n",
    "        \n",
    "        normalize = transforms.Normalize(\n",
    "            mean=(0.4914, 0.4822, 0.4465),\n",
    "            std=(0.2023, 0.1994, 0.2010))\n",
    "        color_jitter = transforms.ColorJitter(0.8 * s, \n",
    "                                              0.8 * s,\n",
    "                                              0.8 * s,\n",
    "                                              0.2 * s)\n",
    "        data_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([color_jitter], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(kernel_size=int(0.1 * size)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        \n",
    "        return data_transforms\n",
    "    \n",
    "    # For BYOL\n",
    "    def get_pretrain_dataset(self):        \n",
    "        # BYOL, 지도 학습에 사용되는 학습 데이터\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            train_dataset = datasets.CIFAR10(self.data_dir,\n",
    "                                             train=True,\n",
    "                                             transform=MultiViewGenerator(\n",
    "                                                 base_transforms=self.get_transform(size=32)),\n",
    "                                             download=True)\n",
    "            \n",
    "            return train_dataset\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    # For Supervised Setting (Transfer Learning)\n",
    "    def get_dataset(self):        \n",
    "        # BYOL, 지도 학습에 사용되는 학습 데이터\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            train_dataset = datasets.CIFAR10(self.data_dir,\n",
    "                                             train=True,\n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "            \n",
    "            return train_dataset\n",
    "        \n",
    "        elif self.mode == 'test':\n",
    "            # 지도 학습 테스트에 사용되는 테스트 데이터\n",
    "            test_dataset = datasets.CIFAR10(self.data_dir,\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=False)\n",
    "            \n",
    "            return test_dataset\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"\n",
    "    blur a single image on CPU\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size: int = None):\n",
    "        radias = kernel_size // 2\n",
    "        kernel_size = radias * 2 + 1\n",
    "        self.blur_h = nn.Conv2d(in_channels=3,\n",
    "                                out_channels=3,\n",
    "                                kernel_size=(kernel_size, 1),\n",
    "                                stride=1,\n",
    "                                padding=0, \n",
    "                                bias=False,\n",
    "                                groups=3)\n",
    "        self.blur_v = nn.Conv2d(in_channels=3,\n",
    "                                out_channels=3,\n",
    "                                kernel_size=(1, kernel_size),\n",
    "                                stride=1,\n",
    "                                padding=0, \n",
    "                                bias=False,\n",
    "                                groups=3)\n",
    "        self.k = kernel_size\n",
    "        self.r = radias\n",
    "        \n",
    "        self.blur = nn.Sequential(\n",
    "            nn.ReflectionPad2d(radias),\n",
    "            self.blur_h,\n",
    "            self.blur_v\n",
    "        )\n",
    "        \n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.tensor_to_pil = transforms.ToPILImage()\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        img = self.pil_to_tensor(img).unsqueeze(0)\n",
    "        \n",
    "        sigma = np.random.uniform(0.1, 2.0)\n",
    "        x = np.arange(-self.r, self.r + 1)\n",
    "        x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))\n",
    "        x = x / x.sum()\n",
    "        x = torch.from_numpy(x).view(1, -1).repeat(3, 1)\n",
    "        \n",
    "        self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))\n",
    "        self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            img = self.blur(img)\n",
    "            img = img.squeeze()\n",
    "        \n",
    "        img = self.tensor_to_pil(img)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    \n",
    "class MultiViewGenerator(object):\n",
    "    \"\"\"\n",
    "    Take two random crops of one image as the query and key.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_transforms):\n",
    "        self.transforms = base_transforms\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        output1 = self.transforms(x)\n",
    "        output2 = self.transforms(x)\n",
    "        \n",
    "        output = [output1, output2]\n",
    "#         output = [transform(x) for transform in self.transforms]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 네트워크 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, base_model: str = None):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.resnet_dict = {\n",
    "            'resnet18': models.resnet18(pretrained=False),\n",
    "            'resnet50': models.resnet50(pretrained=False)\n",
    "        }\n",
    "        \n",
    "        resnet = self.get_basemodel(base_model)\n",
    "        \n",
    "        # Define CNN encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            *list(resnet.children())[:-1])\n",
    "        \n",
    "        # Define MLP Projection\n",
    "        self.projection = MLPHead(in_channels=resnet.fc.in_features,\n",
    "                                  mlp_hidden_size=MLP_HIDDEN_SIZE,\n",
    "                                  projection_size=PROJECTION_SIZE)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.shape[0], h.shape[1])\n",
    "        \n",
    "        return self.projection(h)\n",
    "    \n",
    "    def get_basemodel(self, model_name: str = None):\n",
    " \n",
    "        model = self.resnet_dict[model_name]\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int,\n",
    "                 mlp_hidden_size: int,\n",
    "                 projection_size):\n",
    "        super(MLPHead, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, mlp_hidden_size), \n",
    "            nn.BatchNorm1d(mlp_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(mlp_hidden_size, projection_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 지표 및 BYOL 학습 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output: torch.FloatTensor, \n",
    "             target: torch.LongTensor, \n",
    "             topk: tuple = (1, )):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions\n",
    "    for the specified values of k.\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        \n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        \n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        \n",
    "        return res\n",
    "    \n",
    "\n",
    "class BYOL(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.online_network = kwargs['model'].to(DEVICE)\n",
    "        self.target_network = kwargs['model'].to(DEVICE)\n",
    "        \n",
    "        # Only Online Network\n",
    "        self.predictor = kwargs['predictor'].to(DEVICE)\n",
    "        self.optimizer = kwargs['optimizer']\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def update_target_network_parameters(self):\n",
    "        \"\"\"\n",
    "        Momentum Update of the Target Encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.online_network.parameters(), self.target_network.parameters()):\n",
    "            param_k.data = param_k.data * MOMENTUM + param_q.data * (1. - MOMENTUM)\n",
    "            \n",
    "    @staticmethod\n",
    "    def byolloss(x: torch.FloatTensor, y: torch.FloatTensor):\n",
    "        x = F.normalize(x, dim=1)\n",
    "        y = F.normalize(y, dim=1)\n",
    "        \n",
    "        return 2 - 2 * (x * y).sum(dim=-1)\n",
    "    \n",
    "    def initialize_target_network(self):\n",
    "        \"\"\"\n",
    "        Init Momentum Network as Target Encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.online_network.parameters(), self.target_network.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # Initialize\n",
    "            param_k.requires_grad = False  # Not Update by Gradient\n",
    "            \n",
    "    def update(self, \n",
    "               batch_view_1: torch.FloatTensor, \n",
    "               batch_view_2: torch.FloatTensor):\n",
    "        \n",
    "        # Compute Online Feature\n",
    "        predictions_from_view_1 = self.predictor(self.online_network(batch_view_1))\n",
    "        predictions_from_view_2 = self.predictor(self.online_network(batch_view_2))\n",
    "        \n",
    "        # Compute Target Feature\n",
    "        with torch.no_grad():\n",
    "            targets_to_view_2 = self.target_network(batch_view_1)\n",
    "            targets_to_view_1 = self.target_network(batch_view_2)\n",
    "        \n",
    "        loss = self.byolloss(predictions_from_view_1, targets_to_view_1)\n",
    "        loss += self.byolloss(predictions_from_view_2, targets_to_view_2)\n",
    "        \n",
    "        return loss.mean()        \n",
    "            \n",
    "    def train(self, train_loader):\n",
    "        \n",
    "        n_iter = 0\n",
    "        \n",
    "        # Initialize Target Network\n",
    "        self.initialize_target_network()\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "        \n",
    "            train_loss = 0\n",
    "            for (batch_view_1, batch_view_2), _ in tqdm(train_loader):\n",
    "                batch_view_1 = batch_view_1.to(DEVICE)\n",
    "                batch_view_2 = batch_view_2.to(DEVICE)\n",
    "                \n",
    "                loss = self.update(batch_view_1=batch_view_1, \n",
    "                                   batch_view_2=batch_view_2)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Update Target Encoder\n",
    "                self.update_target_network_parameters()\n",
    "                \n",
    "                train_loss += loss\n",
    "                \n",
    "                if n_iter % LOG_EVERY_N_STEPS == 0:\n",
    "                    print(f'Loss: {loss} \\n')\n",
    "                \n",
    "                n_iter += 1\n",
    "            \n",
    "            train_loss /= (len(train_loader))\n",
    "            \n",
    "            print('=' * 30)\n",
    "            print(f'Epoch: {epoch + 1} \\n'\n",
    "                  f'Loss: {train_loss} \\n')\n",
    "            \n",
    "        return self.online_network               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYOL 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "    # Define Dataset, Dataloader\n",
    "    dataset = CIFAR10Dataset(mode='train')\n",
    "    \n",
    "    train_dataset = dataset.get_pretrain_dataset()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKER,\n",
    "        drop_last=True)\n",
    "    \n",
    "    # Define Model, predictor, optimizer\n",
    "    model = ResNet(\n",
    "        base_model=ARCHITECTURE)\n",
    "    \n",
    "    predictor = MLPHead(\n",
    "        in_channels=PROJECTION_SIZE,\n",
    "        mlp_hidden_size=PROJECTION_SIZE,\n",
    "        projection_size=PROJECTION_SIZE)\n",
    "    \n",
    "    # Optimizing Encoder, predictor\n",
    "    optim_params = list(model.parameters()) + list(predictor.parameters())\n",
    "    optimizer = torch.optim.Adam(\n",
    "        optim_params, \n",
    "        lr=LEARNING_RATE, \n",
    "        weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    with torch.cuda.device(GPU_INDEX):\n",
    "        byol = BYOL(\n",
    "            model=model,\n",
    "            predictor=predictor,\n",
    "            optimizer=optimizer)\n",
    "        \n",
    "        pretrained_model = byol.train(train_loader=train_loader)\n",
    "    \n",
    "    return pretrained_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pretrained_model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYOL로 사전 학습한 인코더를 사용하여 지도 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervised(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.encoder = kwargs['encoder'].to(DEVICE)\n",
    "        self.classifier = kwargs['classifier'].to(DEVICE)\n",
    "        self.optimizer = kwargs['optimizer']\n",
    "        self.criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "        \n",
    "    def train_test(self, train_loader, test_loader):\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            \n",
    "            # Train\n",
    "            top1_train_acc, train_loss = 0, 0\n",
    "            for i, (images, targets) in enumerate(train_loader):\n",
    "                images = images.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                \n",
    "                # Freeze Encoder Parameters\n",
    "                with torch.no_grad():\n",
    "                    logits = self.encoder(images)\n",
    "                    logits = logits.squeeze()\n",
    "                    \n",
    "                logits = self.classifier(logits)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                top1 = accuracy(logits, targets, topk=(1, ))\n",
    "                top1_train_acc += top1[0]\n",
    "                \n",
    "                train_loss += loss\n",
    "            \n",
    "            top1_train_acc /= (i + 1)\n",
    "            train_loss /= (i + 1)\n",
    "            \n",
    "            # Test\n",
    "            top1_test_acc = 0\n",
    "            top5_test_acc = 0\n",
    "            for i, (images, targets) in enumerate(test_loader):\n",
    "                images = images.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = self.encoder(images)\n",
    "                    logits = logits.squeeze()\n",
    "                    logits = self.classifier(logits)\n",
    "                \n",
    "                top1, top5 = accuracy(logits, targets, topk=(1, 5))\n",
    "                top1_test_acc += top1[0]\n",
    "                top5_test_acc += top5[0]\n",
    "            \n",
    "            top1_test_acc /= (i + 1)\n",
    "            top5_test_acc /= (i + 1)\n",
    "\n",
    "            print(f'Epoch: {epoch + 1} \\n'\n",
    "                  f'Loss: {train_loss} \\n'\n",
    "                  f'Top1 Train Accuracy: {top1_train_acc.item()} \\n'\n",
    "                  f'Top1 Test Accuracy: {top1_test_acc.item()} \\n'\n",
    "                  f'Top5 Test Accuracy: {top5_test_acc.item()} \\n')\n",
    "            \n",
    "        return self.encoder, self.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "    # Define Dataset, Dataloader\n",
    "    train_dataset = CIFAR10Dataset(mode='train')\n",
    "    test_dataset = CIFAR10Dataset(mode='test')\n",
    "    \n",
    "    train_dataset = train_dataset.get_dataset()\n",
    "    test_dataset = test_dataset.get_dataset()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKER,\n",
    "        drop_last=False)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKER,\n",
    "        drop_last=False)\n",
    "    \n",
    "    # Define Model, optimizer, linear classifier\n",
    "    encoder = pretrained_model.encoder\n",
    "    \n",
    "    classifier = nn.Linear(512, NUM_CLASS)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        classifier.parameters(), \n",
    "        lr=LEARNING_RATE, \n",
    "        weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    with torch.cuda.device(GPU_INDEX):\n",
    "        supervised = Supervised(\n",
    "            encoder=encoder,\n",
    "            classifier=classifier,\n",
    "            optimizer=optimizer)\n",
    "        \n",
    "        supervised_encoder, supervised_classifier = supervised.train_test(train_loader=train_loader,\n",
    "                                                                          test_loader=test_loader)\n",
    "    \n",
    "    return supervised_encoder, supervised_classifier\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    supervised_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
